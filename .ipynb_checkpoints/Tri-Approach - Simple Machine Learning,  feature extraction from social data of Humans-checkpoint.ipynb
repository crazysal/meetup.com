{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Use Env : Base , NLTK not installed in tensorflow env of conda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import gc\n",
    "gc.collect()\n",
    "reader = csv.reader(open(\"./meetup_sane.csv\"), delimiter=\",\")\n",
    "meetup = list(reader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Text Computation Pre Processor Class\n",
    "###### The transform method \n",
    "takes a list of documents (given as the variable, X) and returns a new list of tokenized documents, where each document is transformed into list of ordered tokens.\n",
    "    \n",
    "###### The tokenize method \n",
    "breaks raw strings into sentences, then breaks those sentences into words and punctuation, \n",
    "and applies a part of speech tag. The token is then normalized: made lower case,\n",
    "then stripped of whitespace and other types of punctuation that may be appended. \n",
    "If the token is a stopword or if every character is punctuation, the token is ignored. \n",
    "If it is not ignored, the part of speech is used to lemmatize the token, which is then yielded\n",
    "\n",
    "                \n",
    "###### The Lemmatization method \n",
    "is the process of looking up a single word form from the variety of morphologic affixes that can be applied to indicate tense, plurality, gender, etc. First we need to identify the WordNet tag form based on the Penn Treebank tag, which is returned from NLTK’s standard pos_tag function. \n",
    "We simply look to see if the Penn tag starts with ‘N’, ‘V’, ‘R’, or ‘J’ and can correctly identify if its a noun, verb, adverb, or adjective. We then use the new tag to look up the lemma in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "                \n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Evaluate\n",
    "###### The next stage is to create the pipeline, train a classifier, then to evaluate it\n",
    "- The model is split into a training and testing set by shuffling the data\n",
    "- The model is trained on the training set, and evaluated on testing.\n",
    "- A new model is then fit on all of the data and saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "mp_col = meetup[0]\n",
    "train_len = int(0.7*len(meetup))\n",
    "test_len = int(0.15*len(meetup)) + train_len\n",
    "mp_train, mp_test, mp_valid = meetup[1:train_len], meetup[train_len:test_len], meetup[test_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26372 5651 5652\n",
      "26372 5651 5652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(mp_train), len(mp_test), len(mp_valid))  \n",
    "mp_train_df,mp_test_df,mp_valid_df = pd.DataFrame(mp_train, columns=mp_col), pd.DataFrame(mp_test, columns=mp_col), pd.DataFrame(mp_valid, columns=mp_col)\n",
    "print(len(mp_train_df), len(mp_test_df), len(mp_valid_df))  \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Build Method  \n",
    "- takes a classifier class or instance (if given a class, it instantiates the classifier with the defaults) and creates the pipeline with that classifier and fits it. \n",
    "- The function times the build process, evaluates it via the classification report that reports precision, recall, and F1. \n",
    "- Then builds a new model on the complete dataset and writes it out to disk\n",
    "*Note that when using the TfidfVectorizer you must make sure that its default preprocessor, normalizer, and tokenizer are all turned off using the identity function and passing None to the other parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_and_evaluate() missing 2 required positional arguments: 'X' and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-df363d0da5d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_and_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSGDClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\salee\\Anaconda3\\lib\\timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(stmt, setup, timer, number, globals)\u001b[0m\n\u001b[0;32m    231\u001b[0m            number=default_number, globals=None):\n\u001b[0;32m    232\u001b[0m     \u001b[1;34m\"\"\"Convenience function to create Timer object and call timeit method.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m def repeat(stmt=\"pass\", setup=\"pass\", timer=default_timer,\n",
      "\u001b[1;32mC:\\Users\\salee\\Anaconda3\\lib\\timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\salee\\Anaconda3\\lib\\timeit.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer, _stmt)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: build_and_evaluate() missing 2 required positional arguments: 'X' and 'y'"
     ]
    }
   ],
   "source": [
    "# import timeit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split as tts\n",
    "\n",
    "# @timeit\n",
    "def build_and_evaluate(X, y, classifier=SGDClassifier, outpath=None, verbose=True):\n",
    "\n",
    "    @timeit\n",
    "    def build(classifier, X, y=None):\n",
    "        \"\"\"\n",
    "        Inner build function that builds a single model.\n",
    "        \"\"\"\n",
    "        if isinstance(classifier, type):\n",
    "            classifier = classifier()\n",
    "\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', NLTKPreprocessor()),\n",
    "            ('vectorizer', TfidfVectorizer(\n",
    "                tokenizer=identity, preprocessor=None, lowercase=False\n",
    "            )),\n",
    "            ('classifier', classifier),\n",
    "        ])\n",
    "\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    # Label encode the targets\n",
    "    labels = LabelEncoder()\n",
    "    y = labels.fit_transform(y)\n",
    "\n",
    "    # Begin evaluation\n",
    "    if verbose: \n",
    "        print(\"Building for evaluation\")\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2)\n",
    "    model, secs = build(classifier, X_train, y_train)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Evaluation model fit in {:0.3f} seconds\".format(secs))\n",
    "        print(\"Classification Report:\\n\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(clsr(y_test, y_pred, target_names=labels.classes_))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Building complete model and saving ...\")\n",
    "    model, secs = build(classifier, X, y)\n",
    "    model.labels_ = labels\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Complete model fit in {:0.3f} seconds\".format(secs))\n",
    "\n",
    "    if outpath:\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        print(\"Model written out to {}\".format(outpath))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
