{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Use Env : Base , NLTK not installed in tensorflow env of conda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import gc\n",
    "gc.collect()\n",
    "reader = csv.reader(open(\"./meetup_sane.csv\"), delimiter=\",\")\n",
    "meetup = list(reader) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Text Computation Pre Processor Class\n",
    "###### The transform method \n",
    "takes a list of documents (given as the variable, X) and returns a new list of tokenized documents, where each document is transformed into list of ordered tokens.\n",
    "    \n",
    "###### The tokenize method \n",
    "breaks raw strings into sentences, then breaks those sentences into words and punctuation, \n",
    "and applies a part of speech tag. The token is then normalized: made lower case,\n",
    "then stripped of whitespace and other types of punctuation that may be appended. \n",
    "If the token is a stopword or if every character is punctuation, the token is ignored. \n",
    "If it is not ignored, the part of speech is used to lemmatize the token, which is then yielded\n",
    "\n",
    "                \n",
    "###### The Lemmatization method \n",
    "is the process of looking up a single word form from the variety of morphologic affixes that can be applied to indicate tense, plurality, gender, etc. First we need to identify the WordNet tag form based on the Penn Treebank tag, which is returned from NLTK’s standard pos_tag function. \n",
    "We simply look to see if the Penn tag starts with ‘N’, ‘V’, ‘R’, or ‘J’ and can correctly identify if its a noun, verb, adverb, or adjective. We then use the new tag to look up the lemma in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "                \n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and Evaluate\n",
    "###### The Build Method  \n",
    "- takes a classifier class or instance (if given a class, it instantiates the classifier with the defaults) and creates the pipeline with that classifier and fits it. \n",
    "- The function times the build process, evaluates it via the classification report that reports precision, recall, and F1. \n",
    "- Then builds a new model on the complete dataset and writes it out to disk\n",
    "\n",
    "*Note that when using the TfidfVectorizer you must make sure that its default preprocessor, normalizer, and tokenizer are all turned off using the identity function and passing None to the other parameters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator function to time functions\n",
    "import time \n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time() - ts\n",
    "#         print('[TimeIt] func: \"{}\" run in {}s'.format(method.__name__, te - ts))\n",
    "        return result, te \n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajnabhandary/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split as tts\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "@timeit\n",
    "def build_and_evaluate(X, y, classifier=SGDClassifier, outpath=None, verbose=True):\n",
    "\n",
    "    @timeit\n",
    "    def build(classifier, X, y=None):\n",
    "        \"\"\"\n",
    "        Inner build function that builds a single model.\n",
    "        \"\"\"\n",
    "        if isinstance(classifier, type):\n",
    "            classifier = classifier()\n",
    "\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', NLTKPreprocessor()),\n",
    "            ('vectorizer', TfidfVectorizer(\n",
    "                tokenizer=identity, preprocessor=None, lowercase=False\n",
    "            )),\n",
    "            ('classifier', classifier),\n",
    "        ])\n",
    "\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    # Label encode the targets\n",
    "    labels = LabelEncoder()\n",
    "    y = labels.fit_transform(y)\n",
    "\n",
    "    # Begin evaluation\n",
    "    if verbose: \n",
    "        print(\"Building for evaluation\")\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2)\n",
    "    model,secs = build(classifier, X_train, y_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Evaluation test model fit in {:0.3f} seconds\".format(secs))\n",
    "        print(\"Classification Report:\\n\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(clsr(y_test, y_pred, target_names=labels.classes_))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Building complete model and saving ...\")\n",
    "    model, secs = build(classifier, X, y)\n",
    "    model.labels_ = labels\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Complete model fit in {:0.3f} seconds\".format(secs))\n",
    "        \n",
    "\n",
    "    if outpath:\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        print(\"Model written out to {}\".format(outpath))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prajnabhandary/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prajnabhandary/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/prajnabhandary/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/prajnabhandary/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### The next stage is to create the pipeline, train a classifier, then to evaluate it\n",
    "- The model is split into a training and testing set by shuffling the data\n",
    "- The model is trained on the training set, and evaluated on testing.\n",
    "- A new model is then fit on all of the data and saved to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenating textual data from groups and events  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = meetup[1:]\n",
    "meetup_df = pd.DataFrame(mp, columns=meetup[0])\n",
    "meetup_df['event_X'] = meetup_df['group_name'].map(str) +\" \" +  meetup_df['category_name'].map(str) +' ' +meetup_df['venue_name'].map(str) +\" \" + meetup_df['group_description'].map(str) +\" \" +  meetup_df['event_description'].map(str)+\" \" +  meetup_df['bio'].map(str)      \n",
    "event_X = meetup_df.as_matrix(['event_X']).ravel() \n",
    "event_Y = meetup_df.as_matrix(['event_name']).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation\n"
     ]
    }
   ],
   "source": [
    "X_training, X_Validation, Y_training, Y_validation = tts(event_X, event_Y, test_size=0.1)\n",
    "\n",
    "PATH = \"simple_ml/model.pickle\"\n",
    "model = build_and_evaluate(X_training, Y_training, outpath=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "member_bio = meetup_df.as_matrix(['bio']).ravel()\n",
    "print(random.choice(member_bio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(PATH, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "\n",
    "yhat = model.predict([random.choice(member_bio)])\n",
    "labels = LabelEncoder()\n",
    "y = labels.fit_transform(event_Y)\n",
    "print(labels.inverse_transform(yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz = meetup_df.loc[meetup_df['bio'] == 'Lifetime student. Interested primarily in learning more about music. I can teach violin, rock climbing, and programming.']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (zzz['event_name'].any == 'December SF FinTech Demo Day!'):\n",
    "    print('sssss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"simple_ml/model.pickle\"\n",
    "with open(PATH, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "y_pred = model.predict(X_Validation)\n",
    "print(clsr(Y_validation, y_pred, target_names=labels.classes_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
